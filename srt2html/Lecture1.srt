0
00:00:00,100 --> 00:00:03,000
Captions Captions proofread by @xiaolai, www.lixiaolai.com

1
00:00:03,320 --> 00:00:06,140
Funding for this program is provided by...

2
00:00:07,760 --> 00:00:09,670
Additional funding provided by...

3
00:00:33,600 --> 00:00:35,100
This is a course about justice

4
00:00:35,100 --> 00:00:37,200
and we begin with a story.

5
00:00:37,600 --> 00:00:40,080
Suppose you're the driver
of a trolley car,

6
00:00:40,320 --> 00:00:42,590
and your trolley car
is hurtling down the track

7
00:00:42,760 --> 00:00:46,040
at 60 miles an hour.
And at the end of the track

8
00:00:46,140 --> 00:00:49,020
you notice five workers
working on the track.

9
00:00:49,530 --> 00:00:51,940
You try to stop
but you can't,

10
00:00:52,169 --> 00:00:53,660
your brakes don't work.

11
00:00:53,760 --> 00:00:56,250
You feel desperate
because you know

12
00:00:56,519 --> 00:00:59,050
that if you crash
into these five workers,

13
00:00:59,199 --> 00:01:01,500
they will all die.

14
00:01:01,709 --> 00:01:03,730
Let's assume
you know that for sure.

15
00:01:04,890 --> 00:01:08,260
And so you feel helpless
until you notice

16
00:01:08,480 --> 00:01:10,800
that there is,
off to the right,

17
00:01:11,320 --> 00:01:14,930
a side track and at the end
of that track,

18
00:01:15,500 --> 00:01:19,000
there is one worker
working on the track.

19
00:01:19,160 --> 00:01:24,540
Your steering wheel works,
so you can turn the trolley car,

20
00:01:24,640 --> 00:01:27,960
if you want to,
onto the side track

21
00:01:28,950 --> 00:01:32,380
killing the one but sparing the five.

22
00:01:33,500 --> 00:01:38,200
Here's our first question:
what's the right thing to do?

23
00:01:39,000 --> 00:01:41,900
What would you do?
Let's take a poll.

24
00:01:42,690 --> 00:01:46,530
How many would turn
the trolley car

25
00:01:46,690 --> 00:01:50,460
onto the side track?
Raise your hands.

26
00:01:52,309 --> 00:01:56,010
How many wouldn't?
How many would go straight ahead?

27
00:01:57,960 --> 00:02:01,260
Keep your hands up those of you
who would go straight ahead.

28
00:02:04,419 --> 00:02:05,970
A handful of people would,

29
00:02:06,119 --> 00:02:07,980
the vast majority would turn.

30
00:02:08,440 --> 00:02:10,740
Let's hear first,
now we need to begin

31
00:02:11,030 --> 00:02:13,260
to investigate the reasons
why you think

32
00:02:13,600 --> 00:02:15,430
it's the right thing to do.

33
00:02:15,530 --> 00:02:20,080
Let's begin with those in the majority
who would turn to go

34
00:02:20,239 --> 00:02:23,600
onto the side track.
Why would you do it?

35
00:02:23,880 --> 00:02:28,540
What would be your reason?
Who's willing to volunteer a reason?

36
00:02:30,120 --> 00:02:31,580
Go ahead. Stand up.

37
00:02:31,760 --> 00:02:34,900
Because it can't be right
to kill five people

38
00:02:35,280 --> 00:02:37,690
when you can only
kill one person instead.

39
00:02:39,870 --> 00:02:42,960
It wouldn't be right
to kill five if you could kill

40
00:02:43,269 --> 00:02:48,260
one person instead.
That's a good reason.

41
00:02:48,910 --> 00:02:53,880
That's a good reason.
Who else?

42
00:02:54,269 --> 00:03:00,490
Does everybody agree
with that reason? Go ahead.

43
00:03:01,549 --> 00:03:04,370
Well I was thinking it's the same reason
on 9/11 with regard

44
00:03:04,660 --> 00:03:08,560
to the people who flew the plane
into the Pennsylvania field

45
00:03:08,760 --> 00:03:11,590
as heroes because they chose
to kill the people on the plane

46
00:03:11,769 --> 00:03:15,430
and not kill more people
in big buildings.

47
00:03:16,470 --> 00:03:19,240
So the principle there
was the same on 9/11.

48
00:03:19,630 --> 00:03:22,530
It's a tragic circumstance
but better to kill one

49
00:03:22,640 --> 00:03:24,990
so that five can live,

50
00:03:25,799 --> 00:03:26,850
is that the reason
most of you had,

51
00:03:27,179 --> 00:03:29,640
those of you
who would turn? Yes?

52
00:03:30,709 --> 00:03:35,180
Let's hear now
from those in the minority,

53
00:03:35,560 --> 00:03:39,850
those who wouldn't turn. Yes.

54
00:03:40,480 --> 00:03:42,470
Well, I think that's
the same type of mentality

55
00:03:42,700 --> 00:03:45,110
that justifies genocide
and totalitarianism.

56
00:03:45,440 --> 00:03:48,030
In order to save
one type of race,

57
00:03:48,260 --> 00:03:49,570
you wipe out the other.

58
00:03:50,560 --> 00:03:52,080
So what would you do
in this case?

59
00:03:52,310 --> 00:03:57,590
You would, to avoid
the horrors of genocide,

60
00:03:58,030 --> 00:04:01,230
you would crash
into the five and kill them?

61
00:04:03,600 --> 00:04:05,700
Presumably, yes.

62
00:04:06,100 --> 00:04:07,560
You would?
-Yeah.

63
00:04:07,880 --> 00:04:11,200
Okay. Who else?
That's a brave answer.

64
00:04:11,359 --> 00:04:12,380
Thank you.

65
00:04:13,630 --> 00:04:18,420
Let's consider
another trolley car case

66
00:04:20,360 --> 00:04:26,060
and see whether those of you
in the majority

67
00:04:27,599 --> 00:04:30,400
want to adhere
to the principle

68
00:04:31,180 --> 00:04:33,660
"better that one should die
so that five should live."

69
00:04:33,920 --> 00:04:35,730
This time you're not the driver
of the trolley car,

70
00:04:35,960 --> 00:04:39,490
you're an onlooker.
You're standing on a bridge

71
00:04:39,590 --> 00:04:41,820
overlooking a trolley car track.

72
00:04:42,650 --> 00:04:45,530
And down the track comes
a trolley car,

73
00:04:46,039 --> 00:04:48,140
at the end of the track
are five workers,

74
00:04:49,750 --> 00:04:52,390
the brakes don't work,
the trolley car

75
00:04:52,680 --> 00:04:55,900
is about to careen
into the five and kill them.

76
00:04:56,000 --> 00:05:00,860
And now, you're not the driver,
you really feel helpless

77
00:05:01,460 --> 00:05:06,010
until you notice
standing next to you,

78
00:05:06,289 --> 00:05:12,390
leaning over the bridge
is a very fat man.

79
00:05:15,669 --> 00:05:22,460
And you could
give him a shove.

80
00:05:23,110 --> 00:05:29,460
He would fall over the bridge
onto the track right in the way

81
00:05:29,669 --> 00:05:33,360
of the trolley car.
He would die

82
00:05:33,690 --> 00:05:35,910
but he would spare the five.

83
00:05:36,490 --> 00:05:42,950
Now, how many would push
the fat man over the bridge?

84
00:05:43,120 --> 00:05:44,860
Raise your hand.

85
00:05:48,440 --> 00:05:50,150
How many wouldn't?

86
00:05:51,349 --> 00:05:55,560
Most people wouldn't.
Here's the obvious question.

87
00:05:55,840 --> 00:06:01,620
What became of the principle
"better to save five lives

88
00:06:01,750 --> 00:06:03,710
even if it means sacrificing one?"

89
00:06:03,810 --> 00:06:07,260
What became of the principle
that almost everyone endorsed

90
00:06:07,469 --> 00:06:10,660
in the first case?
I need to hear from someone

91
00:06:10,760 --> 00:06:13,640
who was in the majority
in both cases.

92
00:06:13,840 --> 00:06:17,160
How do you explain
the difference between the two? Yes.

93
00:06:18,280 --> 00:06:21,030
The second one, I guess,
involves an active choice

94
00:06:21,130 --> 00:06:26,230
of pushing a person down
which I guess that person himself

95
00:06:26,460 --> 00:06:29,890
would otherwise not have been
involved in the situation at all.

96
00:06:30,219 --> 00:06:37,280
And so to choose on his behalf,
I guess, to involve him

97
00:06:37,380 --> 00:06:39,370
in something that he
otherwise would have escaped is,

98
00:06:40,240 --> 00:06:43,180
I guess, more than what
you have in the first case

99
00:06:43,320 --> 00:06:47,260
where the three parties,
the driver and the two sets of workers,

100
00:06:47,590 --> 00:06:50,310
are already, I guess,
in the situation.

101
00:06:50,930 --> 00:06:53,210
But the guy working,
the one on the track

102
00:06:53,390 --> 00:06:56,030
off to the side,
he didn't choose

103
00:06:56,130 --> 00:06:59,710
to sacrifice his life any more
than the fat man did, did he?

104
00:07:02,349 --> 00:07:05,050
That's true, but he was
on the tracks and...

105
00:07:05,200 --> 00:07:09,090
This guy was on the bridge.

106
00:07:10,700 --> 00:07:13,700
Go ahead, you can come back
if you want. All right.

107
00:07:14,049 --> 00:07:17,140
It's a hard question. You did well.
You did very well.

108
00:07:17,500 --> 00:07:18,900
It's a hard question.

109
00:07:19,840 --> 00:07:26,670
Who else can find a way
of reconciling the reaction

110
00:07:26,930 --> 00:07:29,760
of the majority
in these two cases? Yes.

111
00:07:30,120 --> 00:07:33,410
Well, I guess in the first case
where you have the one worker

112
00:07:33,510 --> 00:07:37,510
and the five,
it's a choice between those two

113
00:07:37,739 --> 00:07:39,310
and you have to make
a certain choice and people

114
00:07:39,549 --> 00:07:41,410
are going to die
because of the trolley car,

115
00:07:41,690 --> 00:07:43,310
not necessarily because
of your direct actions.

116
00:07:43,650 --> 00:07:47,490
The trolley car is a runaway thing
and you're making a split second choice.

117
00:07:47,880 --> 00:07:50,550
Whereas pushing the fat man over
is an actual act

118
00:07:50,750 --> 00:07:52,270
of murder on your part.

119
00:07:52,560 --> 00:07:55,570
You have control over that
whereas you may not have control

120
00:07:55,770 --> 00:07:57,290
over the trolley car.

121
00:07:57,760 --> 00:08:00,040
So I think it's a slightly
different situation.

122
00:08:00,549 --> 00:08:04,390
All right, who has a reply?
That's good. Who has a way?

123
00:08:04,780 --> 00:08:07,870
Who wants to reply?
Is that a way out of this?

124
00:08:09,380 --> 00:08:10,790
I don't think that's
a very good reason

125
00:08:10,960 --> 00:08:13,800
because you choose to-
either way you have to choose

126
00:08:13,960 --> 00:08:16,850
who dies because you either
choose to turn and kill the person,

127
00:08:17,010 --> 00:08:19,780
which is an act
of conscious thought to turn,

128
00:08:19,880 --> 00:08:21,970
or you choose to push
the fat man over

129
00:08:22,080 --> 00:08:24,540
which is also an active,
conscious action.

130
00:08:24,870 --> 00:08:26,860
So either way,
you're making a choice.

131
00:08:28,060 --> 00:08:29,740
Do you want to reply?

132
00:08:30,090 --> 00:08:31,820
I'm not really sure
that that's the case.

133
00:08:31,919 --> 00:08:32,860
It just still seems
kind of different.

134
00:08:33,120 --> 00:08:36,130
The act of actually pushing
someone over onto the tracks

135
00:08:36,280 --> 00:08:40,120
and killing him,
you are actually killing him yourself.

136
00:08:40,360 --> 00:08:41,850
You're pushing him
with your own hands.

137
00:08:42,030 --> 00:08:43,290
You're pushing him
and that's different

138
00:08:43,470 --> 00:08:46,020
than steering something
that is going to cause

139
00:08:46,240 --> 00:08:47,940
death into another.

140
00:08:48,199 --> 00:08:51,100
You know, it doesn't really sound right
saying it now.

141
00:08:51,199 --> 00:08:54,200
No, no. It's good. It's good.
What's your name?

142
00:08:54,360 --> 00:08:55,140
Andrew.

143
00:08:55,510 --> 00:08:57,210
Andrew.
Let me ask you this question, Andrew.

144
00:08:57,209 --> 00:08:58,310
Yes.

145
00:08:59,270 --> 00:09:04,940
Suppose standing on the bridge
next to the fat man,

146
00:09:05,310 --> 00:09:07,980
I didn't have to push him,
suppose he was standing over

147
00:09:08,280 --> 00:09:12,260
a trap door that I could open
by turning a steering wheel like that.

148
00:09:17,199 --> 00:09:18,400
Would you turn?

149
00:09:18,860 --> 00:09:22,460
For some reason,
that still just seems more wrong.

150
00:09:22,630 --> 00:09:23,310
Right?

151
00:09:23,600 --> 00:09:26,700
I mean, maybe if you accidentally
like leaned into the steering wheel

152
00:09:27,100 --> 00:09:28,900
or something like that.

153
00:09:29,900 --> 00:09:33,600
But... Or say that
the car is hurtling

154
00:09:33,640 --> 00:09:36,190
towards a switch
that will drop the trap.

155
00:09:38,089 --> 00:09:39,320
Then I could agree with that.

156
00:09:39,439 --> 00:09:40,240
That's all right. Fair enough.

157
00:09:40,300 --> 00:09:44,100
It still seems wrong in a way
that it doesn't seem wrong

158
00:09:44,260 --> 00:09:45,990
in the first case to turn, you say.

159
00:09:46,319 --> 00:09:48,020
And in another way, I mean,
in the first situation

160
00:09:48,120 --> 00:09:50,420
you're involved directly
with the situation.

161
00:09:50,680 --> 00:09:52,490
In the second one,
you're an onlooker as well.

162
00:09:52,500 --> 00:09:54,600
All right. -So you have the choice
of becoming involved or not

163
00:09:54,890 --> 00:09:56,020
by pushing the fat man.

164
00:09:56,100 --> 00:09:59,700
All right. Let's forget for the moment
about this case.

165
00:09:59,800 --> 00:10:02,890
That's good.
Let's imagine a different case.

166
00:10:03,120 --> 00:10:06,140
This time you're a doctor
in an emergency room

167
00:10:06,510 --> 00:10:09,700
and six patients
come to you.

168
00:10:11,810 --> 00:10:15,180
They've been in a terrible
trolley car wreck.

169
00:10:18,740 --> 00:10:20,680
Five of them
sustain moderate injuries,

170
00:10:20,770 --> 00:10:23,050
one is severely injured,
you could spend all day

171
00:10:23,199 --> 00:10:27,420
caring for the one
severely injured victim

172
00:10:27,750 --> 00:10:29,740
but in that time,
the five would die.

173
00:10:29,970 --> 00:10:32,250
Or you could look after the five,
restore them to health

174
00:10:32,420 --> 00:10:35,720
but during that time,
the one severely injured person

175
00:10:35,839 --> 00:10:36,660
would die.

176
00:10:36,839 --> 00:10:40,980
How many would save the five?
Now as the doctor,

177
00:10:41,199 --> 00:10:42,880
how many would save the one?

178
00:10:44,640 --> 00:10:48,390
Very few people,
just a handful of people.

179
00:10:49,560 --> 00:10:53,980
Same reason, I assume.
One life versus five?

180
00:10:55,719 --> 00:10:58,840
Now consider another doctor case.

181
00:10:59,319 --> 00:11:04,010
This time, you're a transplant surgeon
and you have five patients,

182
00:11:04,290 --> 00:11:07,670
each in desperate need
of an organ transplant

183
00:11:07,819 --> 00:11:09,700
in order to survive.

184
00:11:09,959 --> 00:11:13,520
One needs a heart,
one a lung, one a kidney,

185
00:11:13,760 --> 00:11:18,040
one a liver,
and the fifth a pancreas.

186
00:11:19,100 --> 00:11:27,100
And you have no organ donors.
You are about to see them die.

187
00:11:27,829 --> 00:11:32,620
And then it occurs to you
that in the next room

188
00:11:32,709 --> 00:11:36,030
there's a healthy guy
who came in for a check-up.

189
00:11:39,870 --> 00:11:49,100
And he's 每 you like that 每
and he's taking a nap,

190
00:11:53,510 --> 00:11:58,240
you could go in very quietly,
yank out the five organs,

191
00:11:58,400 --> 00:12:02,760
that person would die,
but you could save the five.

192
00:12:03,699 --> 00:12:11,200
How many would do it?
Anyone? How many?

193
00:12:11,579 --> 00:12:13,750
Put your hands up
if you would do it.

194
00:12:18,270 --> 00:12:20,440
Anyone in the balcony?

195
00:12:20,540 --> 00:12:21,280
I would.

196
00:12:21,510 --> 00:12:24,860
You would? Be careful,
don't lean over too much.

197
00:12:27,050 --> 00:12:31,180
How many wouldn't?
All right. What do you say?

198
00:12:31,459 --> 00:12:32,670
Speak up in the balcony,

199
00:12:32,770 --> 00:12:36,090
you who would yank out
the organs. Why?

200
00:12:36,319 --> 00:12:39,070
I'd actually like to explore a
slightly alternate possibility

201
00:12:39,300 --> 00:12:41,820
of just taking the one
of the five who needs an organ

202
00:12:42,170 --> 00:12:45,230
who dies first and using
their four healthy organs

203
00:12:45,329 --> 00:12:47,060
to save the other four.

204
00:12:50,000 --> 00:12:56,800
That's a pretty good idea.
That's a great idea

205
00:12:58,110 --> 00:13:01,170
except for the fact
that you just wrecked

206
00:13:01,449 --> 00:13:04,040
the philosophical point.

207
00:13:05,000 --> 00:13:10,130
Let's step back from these stories
and these arguments

208
00:13:10,569 --> 00:13:14,830
to notice a couple of things
about the way the arguments

209
00:13:14,930 --> 00:13:17,280
have begun to unfold.

210
00:13:17,480 --> 00:13:22,720
Certain moral principles
have already begun to emerge

211
00:13:23,339 --> 00:13:25,880
from the discussions we've had.

212
00:13:26,400 --> 00:13:30,900
And let's consider
what those moral principles look like.

213
00:13:31,540 --> 00:13:34,550
The first moral principle
that emerged in the discussion

214
00:13:34,990 --> 00:13:38,500
said the right thing to do,
the moral thing to do

215
00:13:39,069 --> 00:13:44,690
depends on the consequences
that will result from your action.

216
00:13:45,810 --> 00:13:49,180
At the end of the day,
better that five should live

217
00:13:49,329 --> 00:13:51,610
even if one must die.

218
00:13:52,310 --> 00:13:57,880
That's an example
of consequentialist moral reasoning.

219
00:13:59,420 --> 00:14:01,770
Consequentialist moral reasoning
locates morality

220
00:14:01,870 --> 00:14:04,570
in the consequences of an act,
in the state of the world

221
00:14:04,750 --> 00:14:08,280
that will result from the thing you do.

222
00:14:09,209 --> 00:14:12,560
But then we went a little further,
we considered those other cases

223
00:14:13,110 --> 00:14:19,610
and people weren't so sure
about consequentialist moral reasoning.

224
00:14:20,300 --> 00:14:22,400
When people hesitated

225
00:14:22,949 --> 00:14:26,060
to push the fat man
over the bridge

226
00:14:26,170 --> 00:14:29,730
or to yank out the organs
of the innocent patient,

227
00:14:30,010 --> 00:14:35,510
people gestured toward reasons
having to do with

228
00:14:35,800 --> 00:14:40,200
the intrinsic quality
of the act itself,

229
00:14:40,319 --> 00:14:44,770
consequences be what they may.
People were reluctant.

230
00:14:45,390 --> 00:14:49,050
People thought it was just wrong,
categorically wrong,

231
00:14:49,339 --> 00:14:53,370
to kill a person,
an innocent person,

232
00:14:53,699 --> 00:14:56,870
even for the sake
of saving five lives.

233
00:14:57,150 --> 00:15:01,180
At least people thought
that in the second version

234
00:15:01,280 --> 00:15:06,320
of each story we considered.
So this points

235
00:15:06,760 --> 00:15:15,880
to a second categorical way
of thinking about moral reasoning.

236
00:15:16,270 --> 00:15:19,150
Categorical moral reasoning
locates morality

237
00:15:19,380 --> 00:15:21,890
in certain absolute
moral requirements,

238
00:15:22,069 --> 00:15:26,570
certain categorical duties and rights,
regardless of the consequences.

239
00:15:27,680 --> 00:15:30,900
We're going to explore
in the days and weeks to come

240
00:15:31,130 --> 00:15:34,610
the contrast between
consequentialist and categorical

241
00:15:34,760 --> 00:15:36,650
moral principles.

242
00:15:36,979 --> 00:15:41,580
The most influential example
of consequential moral reasoning

243
00:15:41,819 --> 00:15:44,460
is utilitarianism,
a doctrine invented

244
00:15:44,560 --> 00:15:47,700
by Jeremy Bentham,
the 18th century

245
00:15:47,959 --> 00:15:50,730
English political philosopher.

246
00:15:51,560 --> 00:15:56,710
The most important philosopher
of categorical moral reasoning

247
00:15:56,880 --> 00:16:02,270
is the 18th century
German philosopher Immanuel Kant.

248
00:16:02,839 --> 00:16:05,300
So we will look
at those two different modes

249
00:16:05,589 --> 00:16:08,360
of moral reasoning,
assess them,

250
00:16:08,560 --> 00:16:10,920
and also consider others.

251
00:16:11,069 --> 00:16:13,560
If you look at the syllabus,
you'll notice that we read

252
00:16:13,709 --> 00:16:15,990
a number of great
and famous books,

253
00:16:16,319 --> 00:16:22,020
books by Aristotle, John Locke,
Immanuel Kant, John Stewart Mill,

254
00:16:22,300 --> 00:16:23,790
and others.

255
00:16:24,400 --> 00:16:25,740
You'll notice too
from the syllabus

256
00:16:26,120 --> 00:16:28,290
that we don't only
read these books;

257
00:16:28,500 --> 00:16:34,330
we also take up contemporary,
political, and legal controversies

258
00:16:34,560 --> 00:16:39,870
that raise philosophical questions.
We will debate equality and inequality,

259
00:16:40,089 --> 00:16:44,900
affirmative action, free speech versus
hate speech, same sex marriage,

260
00:16:45,160 --> 00:16:50,620
military conscription,
a range of practical questions. Why?

261
00:16:51,069 --> 00:16:55,150
Not just to enliven
these abstract and distant books

262
00:16:55,240 --> 00:16:58,010
but to make clear,
to bring out what's at stake

263
00:16:58,300 --> 00:17:01,360
in our everyday lives,
including our political lives,

264
00:17:02,800 --> 00:17:05,360
for philosophy.

265
00:17:05,879 --> 00:17:10,140
And so we will read these books
and we will debate these issues,

266
00:17:10,370 --> 00:17:14,630
and we'll see how each informs
and illuminates the other.

267
00:17:15,800 --> 00:17:21,470
This may sound appealing enough,
but here I have to issue a warning.

268
00:17:22,860 --> 00:17:27,330
And the warning is this,
to read these books

269
00:17:28,860 --> 00:17:34,040
in this way as an exercise
in self knowledge,

270
00:17:34,430 --> 00:17:37,540
to read them in this way
carries certain risks,

271
00:17:38,500 --> 00:17:41,850
risks that are both personal
and political,

272
00:17:42,240 --> 00:17:47,520
risks that every student
of political philosophy has known.

273
00:17:48,090 --> 00:17:54,100
These risks spring from the fact
that philosophy teaches us

274
00:17:54,440 --> 00:17:57,840
and unsettles us
by confronting us with

275
00:17:58,070 --> 00:18:03,120
what we already know.
There's an irony.

276
00:18:03,690 --> 00:18:06,390
The difficulty of this course
consists in the fact

277
00:18:06,480 --> 00:18:09,490
that it teaches
what you already know.

278
00:18:09,820 --> 00:18:15,990
It works by taking what we know
from familiar unquestioned settings

279
00:18:16,820 --> 00:18:19,330
and making it strange.

280
00:18:20,530 --> 00:18:25,620
That's how those examples worked,
the hypotheticals with which we began,

281
00:18:25,840 --> 00:18:28,940
with their mix of playfulness
and sobriety.

282
00:18:29,159 --> 00:18:31,860
It's also how these
philosophical books work.

283
00:18:32,080 --> 00:18:37,100
Philosophy estranges us
from the familiar,

284
00:18:37,399 --> 00:18:43,540
not by supplying new information
but by inviting and provoking

285
00:18:43,760 --> 00:18:49,220
a new way of seeing but,
and here's the risk,

286
00:18:49,560 --> 00:18:56,940
once the familiar turns strange,
it's never quite the same again.

287
00:18:57,840 --> 00:19:06,060
Self knowledge is like lost innocence,
however unsettling you find it;

288
00:19:06,480 --> 00:19:11,890
it can never be un-thought
or un-known.

289
00:19:13,669 --> 00:19:19,600
What makes this enterprise difficult
but also riveting

290
00:19:20,070 --> 00:19:26,240
is that moral and political philosophy
is a story and you don't know

291
00:19:26,340 --> 00:19:27,600
where the story will lead.

292
00:19:27,820 --> 00:19:33,910
But what you do know
is that the story is about you.

293
00:19:34,700 --> 00:19:39,410
Those are the personal risks.
Now what of the political risks?

294
00:19:40,290 --> 00:19:44,920
One way of introducing a course
like this would be to promise you

295
00:19:45,139 --> 00:19:48,020
that by reading these books
and debating these issues,

296
00:19:48,179 --> 00:19:51,240
you will become a better,
more responsible citizen;

297
00:19:52,169 --> 00:19:54,810
you will examine the presuppositions
of public policy,

298
00:19:54,960 --> 00:19:58,000
you will hone your political judgment,
you will become a more

299
00:19:58,310 --> 00:20:01,400
effective participant in public affairs.

300
00:20:03,090 --> 00:20:06,570
But this would be a partial
and misleading promise.

301
00:20:06,800 --> 00:20:08,450
Political philosophy,
for the most part,

302
00:20:08,710 --> 00:20:11,010
hasn't worked that way.

303
00:20:11,870 --> 00:20:15,900
You have to allow for the possibility
that political philosophy

304
00:20:15,990 --> 00:20:20,780
may make you a worse citizen
rather than a better one

305
00:20:22,030 --> 00:20:27,130
or at least a worse citizen
before it makes you a better one,

306
00:20:28,120 --> 00:20:32,570
and that's because
philosophy is a distancing,

307
00:20:32,760 --> 00:20:37,700
even debilitating, activity.
And you see this,

308
00:20:37,960 --> 00:20:40,730
going back to Socrates,
there's a dialogue,

309
00:20:40,970 --> 00:20:45,470
the Gorgias, in which
one of Socrates' friends, Callicles,

310
00:20:45,620 --> 00:20:49,340
tries to talk him out of
philosophizing.

311
00:20:50,080 --> 00:20:54,060
Callicles tells Socrates
"Philosophy is a pretty toy

312
00:20:54,629 --> 00:20:56,800
if one indulges in it
with moderation

313
00:20:57,010 --> 00:21:00,800
at the right time of life. But if one
pursues it further than one should,

314
00:21:00,899 --> 00:21:03,780
it is absolute ruin."

315
00:21:03,919 --> 00:21:08,090
"Take my advice," Callicles says,
"abandon argument.

316
00:21:08,730 --> 00:21:11,480
Learn the accomplishments
of active life,

317
00:21:11,820 --> 00:21:14,230
take for your models
not those people who spend

318
00:21:14,460 --> 00:21:19,010
their time on these petty quibbles
but those who have a good livelihood

319
00:21:19,179 --> 00:21:22,210
and reputation and many
other blessings."

320
00:21:22,629 --> 00:21:29,950
So Callicles is really saying to Socrates
"Quit philosophizing, get real,

321
00:21:30,290 --> 00:21:32,880
go to business school."

322
00:21:35,350 --> 00:21:40,740
And Callicles did have a point.
He had a point because philosophy

323
00:21:40,970 --> 00:21:44,970
distances us from conventions,
from established assumptions,

324
00:21:45,310 --> 00:21:47,010
and from settled beliefs.

325
00:21:47,189 --> 00:21:50,170
Those are the risks,
personal and political.

326
00:21:50,320 --> 00:21:51,610
And in the face
of these risks,

327
00:21:51,780 --> 00:21:53,980
there is a characteristic evasion.

328
00:21:54,399 --> 00:21:57,280
The name of the evasion
is skepticism, it's the idea 每

329
00:21:57,560 --> 00:22:02,340
well, it goes something like this 每
we didn't resolve once and for all

330
00:22:02,760 --> 00:22:08,640
either the cases or the principles
we were arguing when we began

331
00:22:09,990 --> 00:22:13,290
and if Aristotle and Locke
and Kant and Mill

332
00:22:13,439 --> 00:22:17,230
haven't solved these questions
after all of these years,

333
00:22:17,439 --> 00:22:21,520
who are we to think that we,
here in Sanders Theatre,

334
00:22:21,800 --> 00:22:25,460
over the course of a semester,
can resolve them?

335
00:22:26,710 --> 00:22:31,440
And so, maybe it's just a matter
of each person having his or her own

336
00:22:31,540 --> 00:22:34,370
principles and there's nothing more
to be said about it,

337
00:22:34,480 --> 00:22:36,690
no way of reasoning.

338
00:22:36,790 --> 00:22:39,540
That's the evasion,
the evasion of skepticism,

339
00:22:39,639 --> 00:22:42,600
to which I would offer
the following reply.

340
00:22:42,750 --> 00:22:47,690
It's true, these questions have been
debated for a very long time

341
00:22:47,840 --> 00:22:51,680
but the very fact
that they have recurred and persisted

342
00:22:52,389 --> 00:22:56,940
may suggest that though
they're impossible in one sense,

343
00:22:57,379 --> 00:22:59,970
they're unavoidable in another.

344
00:23:00,080 --> 00:23:04,150
And the reason they're unavoidable,
the reason they're inescapable

345
00:23:04,379 --> 00:23:09,160
is that we live some answer
to these questions every day.

346
00:23:10,280 --> 00:23:14,780
So skepticism, just throwing up your hands
and giving up on moral reflection

347
00:23:16,320 --> 00:23:17,940
is no solution.

348
00:23:18,350 --> 00:23:22,300
Immanuel Kant described very well
the problem with skepticism

349
00:23:22,590 --> 00:23:24,870
when he wrote
"Skepticism is a resting place

350
00:23:25,149 --> 00:23:27,450
for human reason,
where it can reflect upon

351
00:23:27,649 --> 00:23:30,710
its dogmatic wanderings,
but it is no dwelling place

352
00:23:30,810 --> 00:23:32,880
for permanent settlement."

353
00:23:33,040 --> 00:23:35,810
"Simply to acquiesce in skepticism,"
Kant wrote,

354
00:23:36,090 --> 00:23:40,980
"can never suffice to overcome
the restlessness of reason."

355
00:23:43,010 --> 00:23:45,000
I've tried to suggest
through these stories

356
00:23:45,100 --> 00:23:48,810
and these arguments
some sense of the risks

357
00:23:48,919 --> 00:23:52,060
and temptations,
of the perils and the possibilities.

358
00:23:52,260 --> 00:23:58,190
I would simply conclude by saying
that the aim of this course

359
00:23:58,480 --> 00:24:04,520
is to awaken the restlessness of reason
and to see where it might lead.

360
00:24:04,750 --> 00:24:06,240
Thank you very much.

361
00:24:15,169 --> 00:24:17,470
Like, in a situation that desperate,
you have to do

362
00:24:17,679 --> 00:24:20,740
what you have to do to survive.
-You have to do what you have to do?

363
00:24:20,970 --> 00:24:22,910
You got to do
what you got to do, pretty much.

364
00:24:23,060 --> 00:24:26,490
If you've been going 19 days
without any food, you know,

365
00:24:26,600 --> 00:24:27,920
someone just has
to take the sacrifice.

366
00:24:28,230 --> 00:24:30,530
Someone has to make the sacrifice
and people can survive.

367
00:24:30,820 --> 00:24:32,210
Alright, that's good.
What's your name?

368
00:24:32,490 --> 00:24:35,310
Marcus.
-Marcus, what do you say to Marcus?

369
00:24:40,899 --> 00:24:46,900
Last time,
we started out last time

370
00:24:47,300 --> 00:24:51,010
with some stories,
with some moral dilemmas

371
00:24:51,159 --> 00:24:54,590
about trolley cars
and about doctors

372
00:24:54,879 --> 00:24:58,440
and healthy patients
vulnerable to being victims

373
00:24:58,639 --> 00:25:01,050
of organ transplantation.

374
00:25:01,379 --> 00:25:06,010
We noticed two things
about the arguments we had,

375
00:25:07,179 --> 00:25:09,950
one had to do with the way
we were arguing.

376
00:25:10,290 --> 00:25:13,460
We began with our judgments
in particular cases.

377
00:25:13,870 --> 00:25:19,130
We tried to articulate the reasons
or the principles lying behind

378
00:25:19,220 --> 00:25:21,340
our judgments.

379
00:25:22,750 --> 00:25:25,340
And then confronted
with a new case,

380
00:25:25,669 --> 00:25:29,230
we found ourselves
reexamining those principles,

381
00:25:30,770 --> 00:25:33,410
revising each
in the light of the other.

382
00:25:34,399 --> 00:25:36,160
And we noticed the
built in pressure

383
00:25:36,360 --> 00:25:39,790
to try to bring into alignment
our judgments

384
00:25:39,909 --> 00:25:42,550
about particular cases
and the principles

385
00:25:42,840 --> 00:25:45,400
we would endorse
on reflection.

386
00:25:46,419 --> 00:25:49,430
We also noticed something
about the substance

387
00:25:49,520 --> 00:25:53,180
of the arguments
that emerged from the discussion.

388
00:25:54,510 --> 00:25:58,250
We noticed that sometimes
we were tempted to locate

389
00:25:58,350 --> 00:26:02,300
the morality of an act
in the consequences, in the results,

390
00:26:02,530 --> 00:26:05,460
in the state of the world
that it brought about.

391
00:26:06,710 --> 00:26:10,660
And we called this
consequentialist moral reasoning.

392
00:26:12,090 --> 00:26:15,250
But we also noticed
that in some cases,

393
00:26:16,689 --> 00:26:20,460
we weren't swayed
only by the result.

394
00:26:21,679 --> 00:26:27,090
Sometimes, many of us felt,
that not just consequences

395
00:26:27,300 --> 00:26:29,650
but also the intrinsic quality
or character

396
00:26:29,800 --> 00:26:34,010
of the act matters morally.

397
00:26:35,550 --> 00:26:37,770
Some people argued
that there are certain things

398
00:26:37,879 --> 00:26:42,560
that are just categorically wrong
even if they bring about

399
00:26:43,020 --> 00:26:47,390
a good result,
even if they saved five people

400
00:26:47,540 --> 00:26:49,820
at the cost of one life.

401
00:26:50,129 --> 00:26:56,720
So we contrasted consequentialist
moral principles with categorical ones.

402
00:26:58,360 --> 00:27:03,040
Today and in the next few days,
we will begin to examine

403
00:27:03,189 --> 00:27:10,060
one of the most influential versions
of consequentialist moral theory.

404
00:27:11,310 --> 00:27:14,550
And that's the philosophy
of utilitarianism.

405
00:27:16,250 --> 00:27:18,610
Jeremy Bentham,
the 18th century

406
00:27:18,939 --> 00:27:23,800
English political philosopher
gave first the first clear

407
00:27:23,960 --> 00:27:29,970
systematic expression
to the utilitarian moral theory.

408
00:27:32,470 --> 00:27:38,460
And Bentham's idea,
his essential idea,

409
00:27:38,639 --> 00:27:41,020
is a very simple one.

410
00:27:43,210 --> 00:27:48,540
With a lot of morally
intuitive appeal,

411
00:27:48,770 --> 00:27:51,910
Bentham's idea
is the following,

412
00:27:52,060 --> 00:27:56,840
the right thing to do;
the just thing to do

413
00:27:57,600 --> 00:28:02,700
is to maximize utility.

414
00:28:02,879 --> 00:28:04,370
What did he mean by utility?

415
00:28:06,510 --> 00:28:13,650
He meant by utility
the balance of pleasure over pain,

416
00:28:13,949 --> 00:28:16,150
happiness over suffering.

417
00:28:17,139 --> 00:28:21,820
Here's how he arrived
at the principle of maximizing utility.

418
00:28:22,570 --> 00:28:26,050
He started out by observing
that all of us,

419
00:28:26,699 --> 00:28:31,250
all human beings are governed
by two sovereign masters:

420
00:28:31,639 --> 00:28:33,210
pain and pleasure.

421
00:28:34,929 --> 00:28:39,900
We human beings
like pleasure and dislike pain.

422
00:28:42,639 --> 00:28:47,090
And so we should base morality,
whether we're thinking about

423
00:28:47,389 --> 00:28:52,310
what to do in our own lives
or whether as legislators or citizens,

424
00:28:52,429 --> 00:28:55,310
we're thinking about
what the laws should be.

425
00:28:57,419 --> 00:29:03,670
The right thing to do individually
or collectively is to maximize,

426
00:29:03,949 --> 00:29:09,020
act in a way that maximizes
the overall level of happiness.

427
00:29:11,760 --> 00:29:14,400
Bentham's utilitarianism
is sometimes summed up

428
00:29:14,520 --> 00:29:15,610
with the slogan

429
00:29:15,889 --> 00:29:18,220
"The greatest good
for the greatest number."

430
00:29:19,230 --> 00:29:23,230
With this basic principle
of utility on hand,

431
00:29:23,459 --> 00:29:25,630
let's begin to test it
and to examine it

432
00:29:26,679 --> 00:29:30,060
by turning to another case,
another story, but this time,

433
00:29:30,439 --> 00:29:34,230
not a hypothetical story,
a real life story,

434
00:29:34,330 --> 00:29:38,460
the case of the Queen
versus Dudley and Stevens.

435
00:29:38,560 --> 00:29:41,280
This was a 19th century
British law case

436
00:29:41,439 --> 00:29:46,820
that's famous and much debated
in law schools.

437
00:29:47,679 --> 00:29:51,860
Here's what happened in the case.
I'll summarize the story

438
00:29:52,199 --> 00:29:56,620
then I want to hear
how you would rule,

439
00:29:57,679 --> 00:30:00,320
imagining that you were the jury.

440
00:30:04,060 --> 00:30:08,140
A newspaper account of the time
described the background.

441
00:30:09,199 --> 00:30:12,760
A sadder story of disaster
at sea was never told

442
00:30:13,100 --> 00:30:16,400
than that of the survivors
of the yacht, Mignonette.

443
00:30:16,620 --> 00:30:19,080
The ship floundered
in the South Atlantic,

444
00:30:19,240 --> 00:30:21,410
1300 miles from the cape.

445
00:30:22,209 --> 00:30:25,640
There were four in the crew,
Dudley was the captain,

446
00:30:26,240 --> 00:30:30,140
Stevens was the first mate,
Brooks was a sailor,

447
00:30:30,470 --> 00:30:35,490
all men of excellent character
or so the newspaper account tells us.

448
00:30:35,949 --> 00:30:38,540
The fourth crew member
was the cabin boy,

449
00:30:38,929 --> 00:30:42,230
Richard Parker,
17 years old.

450
00:30:43,159 --> 00:30:46,590
He was an orphan,
he had no family,

451
00:30:47,189 --> 00:30:50,590
and he was on his first
long voyage at sea.

452
00:30:51,780 --> 00:30:53,950
He went,
the news account tells us,

453
00:30:54,110 --> 00:30:56,050
rather against the advice
of his friends.

454
00:30:57,139 --> 00:31:00,100
He went in the hopefulness
of youthful ambition,

455
00:31:00,320 --> 00:31:03,430
thinking the journey
would make a man of him.

456
00:31:03,590 --> 00:31:06,130
Sadly, it was not to be.
The facts of the case

457
00:31:06,360 --> 00:31:07,800
were not in dispute.

458
00:31:07,959 --> 00:31:11,330
A wave hit the ship
and the Mignonette went down.

459
00:31:12,320 --> 00:31:15,200
The four crew members
escaped to a lifeboat.

460
00:31:15,350 --> 00:31:19,870
The only food they had
were two cans of

461
00:31:20,070 --> 00:31:23,440
preserved turnips,
no fresh water.

462
00:31:24,310 --> 00:31:26,610
For the first three days,
they ate nothing.

463
00:31:26,939 --> 00:31:28,930
On the fourth day,
they opened one

464
00:31:29,220 --> 00:31:31,860
of the cans of turnips
and ate it.

465
00:31:32,060 --> 00:31:34,410
The next day
they caught a turtle.

466
00:31:34,600 --> 00:31:37,010
Together with the other
can of turnips,

467
00:31:37,389 --> 00:31:41,160
the turtle enabled them
to subsist for the next few days.

468
00:31:41,439 --> 00:31:44,160
And then for eight days,
they had nothing.

469
00:31:44,320 --> 00:31:46,200
No food. No water.

470
00:31:46,919 --> 00:31:49,960
Imagine yourself
in a situation like that,

471
00:31:50,350 --> 00:31:54,540
what would you do?
Here's what they did.

472
00:31:55,500 --> 00:31:57,820
By now the cabin boy, Parker,
is lying at the bottom

473
00:31:58,159 --> 00:32:00,380
of the lifeboat
in the corner

474
00:32:00,620 --> 00:32:05,170
because he had drunk seawater
against the advice of the others

475
00:32:05,870 --> 00:32:10,320
and he had become ill
and he appeared to be dying.

476
00:32:10,929 --> 00:32:13,650
So on the 19th day,
Dudley, the captain,

477
00:32:13,860 --> 00:32:17,650
suggested that they should all
have a lottery,

478
00:32:17,750 --> 00:32:20,890
that they should draw lots
to see who would die

479
00:32:21,230 --> 00:32:23,170
to save the rest.

480
00:32:23,889 --> 00:32:28,810
Brooks refused.
He didn't like the lottery idea.

481
00:32:29,530 --> 00:32:30,920
We don't know
whether this was

482
00:32:31,199 --> 00:32:33,190
because he didn't want
to take the chance

483
00:32:33,500 --> 00:32:36,950
or because he believed
in categorical moral principles.

484
00:32:37,290 --> 00:32:40,590
But in any case,
no lots were drawn.

485
00:32:42,459 --> 00:32:45,130
The next day
there was still no ship in sight

486
00:32:45,280 --> 00:32:48,340
so Dudley told Brooks
to avert his gaze

487
00:32:48,580 --> 00:32:52,010
and he motioned to Stevens
that the boy, Parker,

488
00:32:52,310 --> 00:32:54,010
had better be killed.

489
00:32:54,350 --> 00:32:58,090
Dudley offered a prayer,
he told the boy his time had come,

490
00:32:58,760 --> 00:33:00,510
and he killed him
with a pen knife,

491
00:33:00,620 --> 00:33:03,290
stabbing him
in the jugular vein.

492
00:33:04,010 --> 00:33:07,100
Brooks emerged
from his conscientious objection

493
00:33:07,379 --> 00:33:09,890
to share
in the gruesome bounty.

494
00:33:10,070 --> 00:33:12,530
For four days,
the three of them fed

495
00:33:12,740 --> 00:33:15,150
on the body and blood
of the cabin boy.

496
00:33:15,480 --> 00:33:18,990
True story.
And then they were rescued.

497
00:33:19,480 --> 00:33:26,900
Dudley describes their rescue
in his diary with staggering euphemism.

498
00:33:27,080 --> 00:33:32,130
"On the 24th day,
as we were having our breakfast,

499
00:33:35,360 --> 00:33:37,770
a ship appeared at last."

500
00:33:38,909 --> 00:33:41,370
The three survivors
were picked up by a German ship.

501
00:33:41,469 --> 00:33:44,450
They were taken back
to Falmouth in England

502
00:33:44,629 --> 00:33:46,800
where they were arrested
and tried.

503
00:33:47,139 --> 00:33:52,190
Brooks turned state's witness.
Dudley and Stevens went to trial.

504
00:33:52,340 --> 00:33:55,690
They didn't dispute the facts.
They claimed they had

505
00:33:55,870 --> 00:33:59,530
acted out of necessity;
that was their defense.

506
00:33:59,810 --> 00:34:03,030
They argued in effect
better that one should die

507
00:34:03,439 --> 00:34:09,300
so that three could survive.
The prosecutor wasn't swayed

508
00:34:09,529 --> 00:34:11,050
by that argument.

509
00:34:11,330 --> 00:34:14,160
He said murder is murder,
and so the case went to trial.

510
00:34:14,360 --> 00:34:18,210
Now imagine you are the jury.
And just to simplify the discussion,

511
00:34:19,319 --> 00:34:24,840
put aside the question of law,
let's assume that you as the jury

512
00:34:26,120 --> 00:34:29,580
are charged with deciding
whether what they did

513
00:34:29,799 --> 00:34:32,780
was morally permissible or not.

514
00:34:34,610 --> 00:34:41,010
How many would vote
'not guilty',

515
00:34:41,120 --> 00:34:44,020
that what they did
was morally permissible?

516
00:34:49,779 --> 00:34:51,930
And how many
would vote 'guilty',

517
00:34:52,029 --> 00:34:54,930
what they did was
morally wrong?

518
00:34:55,319 --> 00:34:57,730
A pretty sizeable majority.

519
00:34:58,380 --> 00:35:01,490
Now let's see what people's reasons are
and let me begin with those

520
00:35:01,720 --> 00:35:03,660
who are in the minority.

521
00:35:04,100 --> 00:35:09,720
Let's hear first from the defense
of Dudley and Stevens.

522
00:35:10,370 --> 00:35:13,140
Why would you morally
exonerate them?

523
00:35:14,549 --> 00:35:17,010
What are your reasons?
Yes.

524
00:35:18,279 --> 00:35:20,610
I think it is morally
reprehensible

525
00:35:20,950 --> 00:35:22,210
but I think that
there is a distinction

526
00:35:22,380 --> 00:35:25,020
between what's morally reprehensible
and what makes someone

527
00:35:25,230 --> 00:35:26,880
legally accountable.

528
00:35:27,089 --> 00:35:28,430
In other words,
as the judge said,

529
00:35:28,759 --> 00:35:31,590
what's always moral
isn't necessarily against the law

530
00:35:31,810 --> 00:35:36,960
and while I don't think
that necessity justifies theft

531
00:35:37,080 --> 00:35:40,650
or murder or any illegal act,
at some point your degree

532
00:35:40,750 --> 00:35:44,670
of necessity does, in fact,
exonerate you from any guilt.

533
00:35:44,950 --> 00:35:49,680
Okay. Good. Other defenders.
Other voices for the defense.

534
00:35:50,779 --> 00:35:55,410
Moral justifications
for what they did. Yes.

535
00:35:56,630 --> 00:35:59,660
Thank you.
I just feel like

536
00:35:59,970 --> 00:36:01,570
in the situation that desperate,
you have to do

537
00:36:01,779 --> 00:36:03,250
what you have to do to survive.

538
00:36:03,450 --> 00:36:05,020
You have to do
what you have to do.

539
00:36:05,120 --> 00:36:06,690
Yeah, you've got to do
what you've got to do.

540
00:36:06,700 --> 00:36:08,000
Pretty much.
If you've been going

541
00:36:08,330 --> 00:36:12,070
19 days without any food, you know,
someone just has to take the sacrifice,

542
00:36:12,279 --> 00:36:14,690
someone has to make the sacrifice
and people can survive.

543
00:36:14,970 --> 00:36:17,320
And furthermore from that,
let's say they survive

544
00:36:17,420 --> 00:36:19,460
and then they become productive
members of society

545
00:36:19,620 --> 00:36:22,240
who go home and start
like a million charity organizations

546
00:36:22,330 --> 00:36:23,540
and this and that
and this and that.

547
00:36:23,540 --> 00:36:25,550
I mean they benefited everybody
in the end. -Yeah.

548
00:36:25,620 --> 00:36:27,090
So, I mean I don't know
what they did afterwards,

549
00:36:27,200 --> 00:36:28,700
they might have gone and like,
I don't know,

550
00:36:28,700 --> 00:36:31,000
killed more people, I don't know.
Whatever but. -What?

551
00:36:31,299 --> 00:36:32,500
Maybe they were assassins.

552
00:36:32,700 --> 00:36:35,600
What if they went home
and they turned out to be assassins?

553
00:36:35,799 --> 00:36:39,300
What if they'd gone home
and turned out to be assassins? Well＃

554
00:36:39,299 --> 00:36:42,000
You'd want to know
who they assassinated.

555
00:36:42,080 --> 00:36:45,980
That's true too. That's fair.
That's fair. I would want to know

556
00:36:46,080 --> 00:36:46,630
who they assassinated.

557
00:36:46,839 --> 00:36:48,150
All right. That's good.
What's your name?

558
00:36:48,299 --> 00:36:50,290
Marcus.
Marcus. All right.

559
00:36:50,960 --> 00:36:53,030
We've heard a defense,
a couple of voices

560
00:36:53,259 --> 00:36:54,340
for the defense.

561
00:36:54,569 --> 00:36:57,210
Now we need to hear
from the prosecution.

562
00:36:57,319 --> 00:37:02,120
Most people think
what they did was wrong. Why?

563
00:37:03,500 --> 00:37:06,740
Yes. -One of the first things
that I was thinking was

564
00:37:07,029 --> 00:37:09,440
they haven't been eating
for a really long time

565
00:37:09,589 --> 00:37:15,600
maybe they're mentally
like affected and so

566
00:37:15,700 --> 00:37:18,210
then that could be used
as a defense,

567
00:37:18,370 --> 00:37:21,040
a possible argument
that they weren't

568
00:37:21,240 --> 00:37:24,300
in the proper state of mind,
they weren't making decisions

569
00:37:24,400 --> 00:37:25,920
they might otherwise be making.

570
00:37:26,069 --> 00:37:29,080
And if that's an appealing argument
that you have to be

571
00:37:29,259 --> 00:37:31,480
in an altered mindset
to do something like that,

572
00:37:31,640 --> 00:37:35,490
it suggests that people
who find that argument convincing

573
00:37:36,049 --> 00:37:38,220
do think that they were
acting immorally.

574
00:37:38,270 --> 00:37:39,030
But what do you-
I want to know

575
00:37:39,109 --> 00:37:40,520
what you think.
You defend them.

576
00:37:40,600 --> 00:37:43,060
I'm sorry, you vote to convict, right?

577
00:37:43,210 --> 00:37:45,430
Yeah, I don't think that
they acted in a morally

578
00:37:45,720 --> 00:37:46,930
appropriate way.

579
00:37:47,020 --> 00:37:48,310
And why not?
What do you say,

580
00:37:48,880 --> 00:37:51,180
here's Marcus,
he just defended them.

581
00:37:51,509 --> 00:37:53,810
He said 每
you heard what he said.

582
00:37:53,900 --> 00:37:55,000
Yes.

583
00:37:55,980 --> 00:37:58,100
That you've got to do
what you've got to do

584
00:37:58,200 --> 00:38:01,680
in a case like that. -Yeah.
-What do you say to Marcus?

585
00:38:05,200 --> 00:38:08,910
That there's
no situation that would allow

586
00:38:09,069 --> 00:38:15,000
human beings to take the idea
of fate or

587
00:38:15,160 --> 00:38:17,330
the other people's lives
in their own hands,

588
00:38:17,480 --> 00:38:20,200
that we don't have
that kind of power.

589
00:38:20,350 --> 00:38:21,920
Good. Okay.
Thank you.

590
00:38:22,080 --> 00:38:22,820
And what's your name?

591
00:38:22,910 --> 00:38:23,700
Britt.

592
00:38:23,880 --> 00:38:28,090
Britt. Okay. Who else?
What do you say? Stand up.

593
00:38:28,299 --> 00:38:32,010
I'm wondering if Dudley and Steven
had asked for Richard Parker's

594
00:38:32,109 --> 00:38:39,060
consent in you know, dying,
if that would exonerate them

595
00:38:39,160 --> 00:38:42,950
from an act of murder
and if so,

596
00:38:43,049 --> 00:38:45,510
is that still morally justifiable?

597
00:38:45,740 --> 00:38:47,230
That's interesting.
All right. Consent.

598
00:38:47,339 --> 00:38:49,230
Wait wait, hang on.
What's your name?

599
00:38:49,380 --> 00:38:50,820
Kathleen.

600
00:38:51,000 --> 00:38:53,410
Kathleen says
suppose they had that,

601
00:38:53,500 --> 00:38:54,810
what would that
scenario look like?

602
00:38:54,990 --> 00:38:58,890
So in the story Dudley is there,
pen knife in hand,

603
00:39:00,710 --> 00:39:04,840
but instead of the prayer
or before the prayer,

604
00:39:04,970 --> 00:39:09,570
he says "Parker, would you mind?"

605
00:39:11,600 --> 00:39:13,400
"We're desperately hungry",

606
00:39:14,560 --> 00:39:19,470
as Marcus empathizes with,
"we're desperately hungry.

607
00:39:20,020 --> 00:39:23,720
You're not going to last long anyhow."
-Yeah. You can be a martyr.

608
00:39:24,009 --> 00:39:27,490
"Would you be a martyr?
How about it Parker?"

609
00:39:29,549 --> 00:39:36,240
Then what do you think?
Would it be morally justified then?

610
00:39:36,470 --> 00:39:41,390
Suppose Parker
in his semi-stupor says "Okay."

611
00:39:43,160 --> 00:39:45,930
I don't think it would be
morally justifiable but I'm wondering if 每

612
00:39:46,040 --> 00:39:47,950
Even then, even then it wouldn't be?
-No.

613
00:39:48,259 --> 00:39:50,640
You don't think that
even with consent

614
00:39:50,870 --> 00:39:52,550
it would be morally justified?

615
00:39:52,850 --> 00:39:55,410
Are there people who think
who want to take up

616
00:39:55,650 --> 00:39:58,290
Kathleen's consent idea
and who think that

617
00:39:58,400 --> 00:40:00,330
that would make it
morally justified?

618
00:40:00,529 --> 00:40:03,430
Raise your hand
if it would, if you think it would.

619
00:40:06,100 --> 00:40:08,980
That's very interesting.
Why would consent

620
00:40:09,359 --> 00:40:15,610
make a moral difference?
Why would it? Yes.

621
00:40:15,799 --> 00:40:17,150
Well, I just think
that if he was making

622
00:40:17,380 --> 00:40:20,020
his own original idea
and it was his idea

623
00:40:20,120 --> 00:40:21,640
to start with,
then that would be

624
00:40:21,930 --> 00:40:24,150
the only situation
in which I would see it

625
00:40:24,359 --> 00:40:26,530
being appropriate in any way
because that way

626
00:40:26,630 --> 00:40:29,640
you couldn't make the argument
that he was pressured,

627
00:40:29,920 --> 00:40:32,380
you know it's three-to-one
or whatever the ratio was.

628
00:40:32,660 --> 00:40:35,260
Right. -And I think that if he was
making a decision

629
00:40:35,359 --> 00:40:37,630
to give his life
and he took on the agency

630
00:40:38,100 --> 00:40:40,090
to sacrifice himself
which some people

631
00:40:40,290 --> 00:40:43,560
might see as admirable
and other people might disagree

632
00:40:43,900 --> 00:40:45,210
with that decision.

633
00:40:45,910 --> 00:40:48,870
So if he came up
with the idea,

634
00:40:49,279 --> 00:40:51,270
that's the only kind
of consent we could have

635
00:40:51,440 --> 00:40:54,480
confidence in morally
then it would be okay.

636
00:40:55,230 --> 00:40:59,960
Otherwise, it would be kind of
coerced consent

637
00:41:00,200 --> 00:41:02,560
under the circumstances,
you think.

638
00:41:05,580 --> 00:41:10,830
Is there anyone who thinks
that even the consent of Parker

639
00:41:11,060 --> 00:41:18,070
would not justify their killing him?
Who thinks that? Yes.

640
00:41:18,509 --> 00:41:19,980
Tell us why. Stand up.

641
00:41:20,069 --> 00:41:22,780
I think that Parker
would be killed with the hope

642
00:41:23,130 --> 00:41:27,030
that the other crew members
would be rescued so there's no

643
00:41:27,130 --> 00:41:28,580
definite reason that
he should be killed

644
00:41:28,960 --> 00:41:32,620
because you don't know
when they're going to get rescued

645
00:41:32,720 --> 00:41:34,420
so if you kill him,
it's killing him in vain,

646
00:41:34,520 --> 00:41:36,930
do you keep killing a crew member
until you're rescued

647
00:41:37,029 --> 00:41:38,920
and then you're left with no one
because someone's going

648
00:41:39,069 --> 00:41:40,150
to die eventually?

649
00:41:40,279 --> 00:41:44,160
Well, the moral logic
of the situation seems to be that,

650
00:41:44,500 --> 00:41:48,350
that they would keep on
picking off the weakest maybe,

651
00:41:48,600 --> 00:41:52,340
one by one,
until they were rescued.

652
00:41:52,440 --> 00:41:55,140
And in this case, luckily,
they were rescued when three at least

653
00:41:55,279 --> 00:42:01,400
were still alive.
Now, if Parker did give his consent,

654
00:42:01,640 --> 00:42:03,730
would it be all right,
do you think or not?

655
00:42:03,960 --> 00:42:06,920
No, it still wouldn't be right.
-And tell us why

656
00:42:07,080 --> 00:42:08,140
it wouldn't be all right.

657
00:42:08,430 --> 00:42:11,860
First of all, cannibalism,
I believe, is morally incorrect

658
00:42:12,319 --> 00:42:14,730
so you shouldn't be
eating human anyway.

659
00:42:15,060 --> 00:42:19,980
So cannibalism is morally
objectionable as such so then,

660
00:42:20,080 --> 00:42:23,980
even on the scenario of
waiting until someone died,

661
00:42:24,359 --> 00:42:26,250
still it would be objectionable.

662
00:42:26,350 --> 00:42:30,300
Yes, to me personally,
I feel like it all depends

663
00:42:30,710 --> 00:42:35,020
on one's personal morals
and like we can't sit here and just,

664
00:42:35,359 --> 00:42:37,460
like this is just my opinion,
of course other people

665
00:42:37,680 --> 00:42:39,390
are going to disagree, but 每

666
00:42:39,490 --> 00:42:41,540
Well we'll see,
let's see what their disagreements are

667
00:42:41,799 --> 00:42:44,620
and then we'll see
if they have reasons that can

668
00:42:44,839 --> 00:42:46,340
persuade you or not.

669
00:42:46,560 --> 00:42:48,300
Let's try that. All right.

670
00:42:48,480 --> 00:42:54,660
Now, is there someone
who can explain,

671
00:42:55,060 --> 00:42:57,380
those of you who are
tempted by consent,

672
00:42:58,000 --> 00:43:02,060
can you explain why
consent makes such

673
00:43:02,319 --> 00:43:03,140
a moral difference?

674
00:43:03,400 --> 00:43:07,340
What about the lottery idea?
Does that count as consent?

675
00:43:07,440 --> 00:43:11,100
Remember at the beginning,
Dudley proposed a lottery,

676
00:43:11,380 --> 00:43:15,330
suppose that they had agreed
to a lottery,

677
00:43:15,609 --> 00:43:22,120
then how many would then say
it was all right?

678
00:43:22,299 --> 00:43:25,310
Suppose there were a lottery,
cabin boy lost,

679
00:43:25,440 --> 00:43:27,790
and the rest of the story unfolded,
then how many people would say

680
00:43:28,020 --> 00:43:30,060
it was morally permissible?

681
00:43:33,359 --> 00:43:35,630
So the numbers are rising
if we had a lottery.

682
00:43:35,859 --> 00:43:38,370
Let's hear from one of you
for whom the lottery

683
00:43:38,470 --> 00:43:42,970
would make a moral difference.
Why would it?

684
00:43:43,960 --> 00:43:46,390
I think the essential element,
in my mind,

685
00:43:46,540 --> 00:43:49,420
that makes it a crime
is the idea that they decided

686
00:43:49,650 --> 00:43:52,950
at some point that their lives
were more important than his,

687
00:43:53,180 --> 00:43:56,580
and that, I mean, that's kind of
the basis for really any crime.

688
00:43:56,940 --> 00:43:59,640
Right? It's like my needs,
my desires are more important

689
00:43:59,790 --> 00:44:02,070
than yours and mine
take precedent.

690
00:44:02,190 --> 00:44:04,650
And if they had done a lottery
where everyone consented

691
00:44:04,930 --> 00:44:07,700
that someone should die
and it's sort of like they're all

692
00:44:07,910 --> 00:44:10,840
sacrificing themselves
to save the rest.

693
00:44:11,310 --> 00:44:13,200
Then it would be all right?

694
00:44:13,290 --> 00:44:17,530
A little grotesque but每.
-But morally permissible?

695
00:44:17,759 --> 00:44:20,110
Yes.
-And what's your name?

696
00:44:20,319 --> 00:44:24,060
Matt.
-So Matt, for you,

697
00:44:25,940 --> 00:44:28,610
what bothers you is
not the cannibalism

698
00:44:28,700 --> 00:44:30,900
but the lack of due process.

699
00:44:31,859 --> 00:44:33,670
I guess you could say that.

700
00:44:33,950 --> 00:44:40,090
Right? And can someone who agrees
with Matt say a little bit more

701
00:44:40,460 --> 00:44:48,540
about why a lottery would make it,
in your view, morally permissible.

702
00:44:49,600 --> 00:44:50,700
Go ahead.

703
00:44:50,799 --> 00:44:52,800
The way I understood it
originally was that

704
00:44:52,799 --> 00:44:54,800
that was the whole issue
is that the cabin boy

705
00:44:54,900 --> 00:44:57,540
was never consulted
about whether or not

706
00:44:57,750 --> 00:44:58,850
something was going
to happen to him,

707
00:44:58,980 --> 00:45:01,440
even with the original lottery
whether or not

708
00:45:01,670 --> 00:45:04,440
he would be a part of that,
it was just decided

709
00:45:04,650 --> 00:45:06,220
that he was the one
that was going to die.

710
00:45:06,629 --> 00:45:08,070
Right, that's what happened
in the actual case.

711
00:45:08,299 --> 00:45:08,910
Right.

712
00:45:09,140 --> 00:45:11,230
But if there were a lottery
and they'd all agreed to the procedure,

713
00:45:11,650 --> 00:45:13,560
you think that would be okay?

714
00:45:13,660 --> 00:45:16,560
Right, because then everyone
knows that there's going to be a death,

715
00:45:16,660 --> 00:45:21,080
whereas the cabin boy didn't know that
this discussion was even happening,

716
00:45:21,240 --> 00:45:24,330
there was no forewarning
for him to know that

717
00:45:24,420 --> 00:45:26,640
"Hey, I may be the one that's dying."

718
00:45:26,759 --> 00:45:29,020
All right.
Now, suppose everyone agrees

719
00:45:29,200 --> 00:45:31,900
to the lottery, they have the lottery,
the cabin boy loses,

720
00:45:32,129 --> 00:45:34,170
and he changes his mind.

721
00:45:34,870 --> 00:45:37,280
You've already decided,
it's like a verbal contract.

722
00:45:37,560 --> 00:45:39,320
You can't go back on that,
you've decided,

723
00:45:39,520 --> 00:45:41,040
the decision was made.

724
00:45:41,140 --> 00:45:45,100
If you know that you're dying
for the reason of others to live.

725
00:45:45,480 --> 00:45:48,490
If someone else had died,
you know that you would

726
00:45:48,720 --> 00:45:50,190
consume them so 每

727
00:45:51,220 --> 00:45:55,530
Right. But then you could say,
"I know, but I lost".

728
00:45:57,020 --> 00:45:58,590
I just think that
that's the whole moral issue

729
00:45:58,799 --> 00:46:01,160
is that there was no consulting
of the cabin boy

730
00:46:01,359 --> 00:46:04,450
and that's what makes it
the most horrible

731
00:46:04,779 --> 00:46:07,240
is that he had no idea
what was even going on.

732
00:46:07,420 --> 00:46:09,100
That had he known
what was going on,

733
00:46:09,250 --> 00:46:12,260
it would be a bit more
understandable.

734
00:46:12,359 --> 00:46:14,540
All right. Good.
Now I want to hear 每

735
00:46:14,640 --> 00:46:18,890
so there are some who think
it's morally permissible

736
00:46:18,990 --> 00:46:25,260
but only about 20%,
led by Marcus.

737
00:46:26,799 --> 00:46:30,120
Then there are some who say
the real problem here

738
00:46:30,410 --> 00:46:33,890
is the lack of consent,
whether the lack of consent

739
00:46:34,170 --> 00:46:39,710
to a lottery, to a fair procedure or,
Kathleen's idea,

740
00:46:39,940 --> 00:46:44,100
lack of consent
at the moment of death.

741
00:46:45,190 --> 00:46:50,470
And if we add consent,
then more people are willing

742
00:46:50,700 --> 00:46:54,100
to consider the sacrifice
morally justified.

743
00:46:54,879 --> 00:46:57,890
I want to hear now, finally,
from those of you

744
00:46:57,990 --> 00:47:01,440
who think even with consent,
even with a lottery,

745
00:47:01,569 --> 00:47:06,900
even with a final murmur
of consent by Parker,

746
00:47:07,339 --> 00:47:12,020
at the very last moment,
it would still be wrong.

747
00:47:12,960 --> 00:47:16,200
And why would it be wrong?
That's what I want to hear. Yes.

748
00:47:17,270 --> 00:47:20,200
Well, the whole time
I've been leaning off towards

749
00:47:20,379 --> 00:47:26,520
the categorical moral reasoning
and I think that there's a possibility

750
00:47:26,799 --> 00:47:28,220
I'd be okay with the idea
of a lottery

751
00:47:28,480 --> 00:47:33,000
and then the loser taking into
their own hands to kill themselves

752
00:47:33,259 --> 00:47:35,750
so there wouldn't be
an act of murder,

753
00:47:35,950 --> 00:47:39,320
but I still think that
even that way, it's coerced.

754
00:47:39,529 --> 00:47:41,940
Also, I don't think that
there is any remorse,

755
00:47:42,109 --> 00:47:44,990
like in Dudley's diary,
"We're eating our breakfast,'

756
00:47:45,220 --> 00:47:48,020
it seems as though he's just
sort of like, you know,

757
00:47:48,120 --> 00:47:51,490
the whole idea of
not valuing someone else's life.

758
00:47:51,730 --> 00:47:54,890
So that makes me feel
like I have to take the 每

759
00:47:55,490 --> 00:47:58,840
You want to throw the book
at him when he lacks remorse

760
00:47:59,120 --> 00:48:00,820
or a sense of having done
anything wrong.

761
00:48:01,020 --> 00:48:02,460
Right.

762
00:48:02,750 --> 00:48:07,020
So, all right. Good.
Are there any other defenders

763
00:48:07,420 --> 00:48:11,550
who say it's just categorically wrong,
with or without consent?

764
00:48:11,890 --> 00:48:13,330
Yes. Stand up. Why?

765
00:48:13,620 --> 00:48:15,840
I think undoubtedly
the way our society is shaped

766
00:48:16,069 --> 00:48:17,410
murder is murder.

767
00:48:17,560 --> 00:48:18,790
Murder is murder
in every way

768
00:48:19,000 --> 00:48:21,640
and our society looks at murder
down on the same light

769
00:48:21,870 --> 00:48:23,440
and I don't think
it's any different in any case.

770
00:48:23,879 --> 00:48:27,800
Good. Let me ask you a question.
There were three lives at stake versus one.

771
00:48:28,140 --> 00:48:29,290
Okay.

772
00:48:29,730 --> 00:48:33,100
The one, the cabin boy,
he had no family,

773
00:48:33,440 --> 00:48:36,450
he had no dependents,
these other three had families

774
00:48:36,600 --> 00:48:38,490
back home in England;
they had dependents;

775
00:48:38,770 --> 00:48:43,220
they had wives and children.
Think back to Bentham.

776
00:48:43,470 --> 00:48:44,810
Bentham says
we have to consider

777
00:48:45,040 --> 00:48:49,170
the welfare, the utility,
the happiness of everybody.

778
00:48:49,200 --> 00:48:52,400
We have to add it all up
so it's not just numbers,

779
00:48:52,589 --> 00:48:56,070
three against one;
it's also all of those

780
00:48:56,170 --> 00:48:59,000
people at home.

781
00:48:59,170 --> 00:49:01,760
In fact, the London newspaper
at that time and popular opinion

782
00:49:02,100 --> 00:49:05,580
sympathized with them,
Dudley and Stevens,

783
00:49:05,859 --> 00:49:08,190
and the paper said
if they weren't motivated

784
00:49:08,370 --> 00:49:12,030
by affection and concern
for their loved ones at home

785
00:49:12,129 --> 00:49:14,030
and their dependents,
surely they wouldn't have done this.

786
00:49:14,220 --> 00:49:15,440
Yeah and how is that
any different

787
00:49:15,470 --> 00:49:18,440
from people on a corner trying,
with the same desire

788
00:49:18,500 --> 00:49:20,200
to feed their family.
I don't think it's any different.

789
00:49:20,410 --> 00:49:22,580
I think in any case,
if I'm murdering you

790
00:49:22,810 --> 00:49:24,510
to advance my status,
that's murder,

791
00:49:24,640 --> 00:49:26,060
and I think that
we should look at all that

792
00:49:26,290 --> 00:49:27,680
in the same light
instead of criminalizing

793
00:49:27,779 --> 00:49:31,810
certain activities and
making certain things

794
00:49:32,060 --> 00:49:34,700
seem more violently savage
when in the same case,

795
00:49:34,940 --> 00:49:37,710
it's all the same, it's all the same
act and mentality that goes

796
00:49:37,960 --> 00:49:40,130
into murder,
necessity to feed your family so 每

797
00:49:40,259 --> 00:49:44,210
Suppose it weren't three,
suppose it were 30? 300?

798
00:49:44,910 --> 00:49:49,560
One life to save 300?
Or in wartime? 3000?

799
00:49:49,690 --> 00:49:51,420
Suppose the stakes are even bigger.

800
00:49:51,560 --> 00:49:53,170
Suppose the stakes
are even bigger?

801
00:49:53,270 --> 00:49:54,610
I think it's still the same deal.

802
00:49:54,940 --> 00:49:57,220
You think Bentham is wrong
to say the right thing to do

803
00:49:57,819 --> 00:50:01,060
is to add up the
collective happiness?

804
00:50:01,319 --> 00:50:02,660
You think he's wrong
about that?

805
00:50:02,810 --> 00:50:04,700
I don't think he's wrong
but I think murder is murder

806
00:50:05,029 --> 00:50:05,870
in any case.

807
00:50:06,230 --> 00:50:07,020
Well, then Bentham has
to be wrong.

808
00:50:07,299 --> 00:50:08,640
If you're right, he's wrong.

809
00:50:08,790 --> 00:50:10,470
Okay, then he's wrong. I'm right.

810
00:50:10,770 --> 00:50:13,330
All right. Thank you.
Well done. All right.

811
00:50:13,440 --> 00:50:20,520
Let's step back from this discussion
and notice how many objections

812
00:50:20,680 --> 00:50:23,270
have we heard
to what they did?

813
00:50:23,730 --> 00:50:26,350
We heard some defenses
of what they did.

814
00:50:26,450 --> 00:50:30,950
The defenses had to do with necessity,
their dire circumstance and,

815
00:50:32,140 --> 00:50:36,090
implicitly at least,
the idea that numbers matter.

816
00:50:36,200 --> 00:50:40,450
And not only numbers matter
but the wider effects matter;

817
00:50:40,549 --> 00:50:43,300
their families back home,
their dependents.

818
00:50:43,660 --> 00:50:46,430
Parker was an orphan,
no one would miss him.

819
00:50:48,210 --> 00:50:53,650
So if you add up,
if you try to calculate the balance

820
00:50:53,819 --> 00:50:57,900
of happiness and suffering,
you might have a case

821
00:50:58,160 --> 00:51:01,040
for saying what they did
was the right thing.

822
00:51:02,940 --> 00:51:06,910
Then we heard at least
three different types of objections.

823
00:51:09,629 --> 00:51:12,320
We heard an objection
that said what they did

824
00:51:12,420 --> 00:51:15,380
was categorically wrong,
like here at the end,

825
00:51:15,560 --> 00:51:18,260
categorically wrong,
murder is murder,

826
00:51:18,350 --> 00:51:22,010
it's always wrong even if
it increases the overall

827
00:51:22,220 --> 00:51:27,630
happiness of society,
a categorical objection.

828
00:51:28,799 --> 00:51:34,680
But we still need to investigate
why murder is categorically wrong.

829
00:51:35,779 --> 00:51:42,210
Is it because even cabin boys
have certain fundamental rights?

830
00:51:42,700 --> 00:51:45,840
And if that's the reason,
where do those rights come from

831
00:51:46,040 --> 00:51:49,130
if not from some idea
of the larger welfare

832
00:51:49,410 --> 00:51:51,030
or utility or happiness?

833
00:51:51,240 --> 00:51:57,120
Question number one.
Others said a lottery

834
00:51:57,270 --> 00:52:00,830
would make a difference,
a fair procedure Matt said,

835
00:52:04,270 --> 00:52:08,950
and some people
were swayed by that.

836
00:52:09,060 --> 00:52:12,170
That's not a categorical
objection exactly.

837
00:52:12,319 --> 00:52:16,480
It's saying everybody
has to be counted as an equal

838
00:52:16,870 --> 00:52:19,930
even though at the end of the day,
one can be sacrificed

839
00:52:20,310 --> 00:52:23,160
for the general welfare.

840
00:52:23,549 --> 00:52:26,090
That leaves us with
another question to investigate.

841
00:52:26,350 --> 00:52:29,250
Why does agreement
to a certain procedure,

842
00:52:29,879 --> 00:52:33,960
even a fair procedure,
justify whatever result flows

843
00:52:34,180 --> 00:52:37,420
from the operation
of that procedure?

844
00:52:38,230 --> 00:52:42,360
Question number two.
And question number three,

845
00:52:42,700 --> 00:52:47,090
the basic idea of consent.
Kathleen got us on to this.

846
00:52:48,580 --> 00:52:56,050
If the cabin boy had agreed himself,
and not under duress, as was added,

847
00:52:57,220 --> 00:53:00,830
then it would be all right
to take his life to save the rest

848
00:53:01,640 --> 00:53:04,940
and even more people
signed on to that idea.

849
00:53:05,270 --> 00:53:08,460
But that raises a third
philosophical question:

850
00:53:08,819 --> 00:53:13,660
What is the moral work
that consent does?

851
00:53:14,730 --> 00:53:19,040
Why does an act of consent
make such a moral difference,

852
00:53:19,430 --> 00:53:20,840
that an act that would be wrong,

853
00:53:21,149 --> 00:53:27,870
taking a life without consent,
is morally permissible with consent?

854
00:53:29,879 --> 00:53:32,340
To investigate those three questions,
we're going to have to read

855
00:53:32,700 --> 00:53:34,220
some philosophers.

856
00:53:34,549 --> 00:53:37,850
And starting next time,
we're going to read Bentham

857
00:53:37,960 --> 00:53:40,720
and John Stuart Mill,
utilitarian philosophers.

858
00:53:43,960 --> 00:53:45,740
Don't miss the chance
to interact online

859
00:53:45,839 --> 00:53:48,380
with other viewers of Justice.
Join the conversation,

860
00:53:48,609 --> 00:53:50,970
take a pop quiz,
watch lectures you've missed

861
00:53:51,060 --> 00:53:53,890
and learn a lot more.
Visit JusticeHarvard.org.

862
00:53:54,140 --> 00:53:55,530
it's the right thing to do.

863
00:54:36,310 --> 00:54:39,370
Funding for this program
is provided by...

864
00:54:40,170 --> 00:54:43,620
Additional funding provided by...

